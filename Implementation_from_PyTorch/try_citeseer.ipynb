{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Classification with Graph Convolutional Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-22 09:27:35--  https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
      "Resolving linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)... 128.114.47.74\n",
      "Connecting to linqs-data.soe.ucsc.edu (linqs-data.soe.ucsc.edu)|128.114.47.74|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168052 (164K) [application/x-gzip]\n",
      "Saving to: ‘cora.tgz’\n",
      "\n",
      "cora.tgz            100%[===================>] 164.11K   154KB/s    in 1.1s    \n",
      "\n",
      "2024-07-22 09:27:41 (154 KB/s) - ‘cora.tgz’ saved [168052/168052]\n",
      "\n",
      "cora/\n",
      "cora/README\n",
      "cora/cora.cites\n",
      "cora/cora.content\n"
     ]
    }
   ],
   "source": [
    "!wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "!tar -xvzf cora.tgz\n",
    "!rm -r cora.tgz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "dataset = Planetoid(root = 'Citeseer', name = 'Citeseer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.7864, Train Acc: 0.8917, Val Acc: 0.4940, Test Acc: 0.5070\n",
      "Epoch: 010, Loss: 0.1647, Train Acc: 1.0000, Val Acc: 0.6800, Test Acc: 0.6680\n",
      "Epoch: 020, Loss: 0.0193, Train Acc: 1.0000, Val Acc: 0.6720, Test Acc: 0.6650\n",
      "Epoch: 030, Loss: 0.0072, Train Acc: 1.0000, Val Acc: 0.6760, Test Acc: 0.6580\n",
      "Epoch: 040, Loss: 0.0067, Train Acc: 1.0000, Val Acc: 0.6740, Test Acc: 0.6720\n",
      "Epoch: 050, Loss: 0.0090, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6790\n",
      "Epoch: 060, Loss: 0.0113, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6760\n",
      "Epoch: 070, Loss: 0.0121, Train Acc: 1.0000, Val Acc: 0.6920, Test Acc: 0.6800\n",
      "Epoch: 080, Loss: 0.0116, Train Acc: 1.0000, Val Acc: 0.6920, Test Acc: 0.6790\n",
      "Epoch: 090, Loss: 0.0108, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.6820\n",
      "Epoch: 100, Loss: 0.0103, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.6830\n",
      "Epoch: 110, Loss: 0.0097, Train Acc: 1.0000, Val Acc: 0.6900, Test Acc: 0.6830\n",
      "Epoch: 120, Loss: 0.0093, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.6830\n",
      "Epoch: 130, Loss: 0.0089, Train Acc: 1.0000, Val Acc: 0.6900, Test Acc: 0.6820\n",
      "Epoch: 140, Loss: 0.0085, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6840\n",
      "Epoch: 150, Loss: 0.0082, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6850\n",
      "Epoch: 160, Loss: 0.0079, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6820\n",
      "Epoch: 170, Loss: 0.0077, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6830\n",
      "Epoch: 180, Loss: 0.0074, Train Acc: 1.0000, Val Acc: 0.6860, Test Acc: 0.6830\n",
      "Epoch: 190, Loss: 0.0072, Train Acc: 1.0000, Val Acc: 0.6860, Test Acc: 0.6840\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Step 1: Load the Citeseer Dataset\n",
    "dataset = Planetoid(root = 'Citeseer', name = 'Citeseer')\n",
    "# Step 2: Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and other utilities\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Step 3: Training Loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Step 4: Test Function\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data), []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "# Training and Evaluation\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 11.0108, Train Acc: 0.1583, Val Acc: 0.1720, Test Acc: 0.1960\n",
      "Epoch: 010, Loss: 1.7516, Train Acc: 0.7000, Val Acc: 0.2840, Test Acc: 0.3230\n",
      "Epoch: 020, Loss: 0.2621, Train Acc: 0.9333, Val Acc: 0.3560, Test Acc: 0.4030\n",
      "Epoch: 030, Loss: 0.0431, Train Acc: 1.0000, Val Acc: 0.3860, Test Acc: 0.4190\n",
      "Epoch: 040, Loss: 0.0243, Train Acc: 1.0000, Val Acc: 0.3880, Test Acc: 0.4240\n",
      "Epoch: 050, Loss: 0.0183, Train Acc: 1.0000, Val Acc: 0.3960, Test Acc: 0.4290\n",
      "Epoch: 060, Loss: 0.0168, Train Acc: 1.0000, Val Acc: 0.3880, Test Acc: 0.4300\n",
      "Epoch: 070, Loss: 0.0167, Train Acc: 1.0000, Val Acc: 0.3800, Test Acc: 0.4250\n",
      "Epoch: 080, Loss: 0.0170, Train Acc: 1.0000, Val Acc: 0.3860, Test Acc: 0.4260\n",
      "Epoch: 090, Loss: 0.0170, Train Acc: 1.0000, Val Acc: 0.3920, Test Acc: 0.4270\n",
      "Epoch: 100, Loss: 0.0167, Train Acc: 1.0000, Val Acc: 0.3960, Test Acc: 0.4240\n",
      "Epoch: 110, Loss: 0.0162, Train Acc: 1.0000, Val Acc: 0.3960, Test Acc: 0.4240\n",
      "Epoch: 120, Loss: 0.0156, Train Acc: 1.0000, Val Acc: 0.4060, Test Acc: 0.4240\n",
      "Epoch: 130, Loss: 0.0150, Train Acc: 1.0000, Val Acc: 0.4160, Test Acc: 0.4300\n",
      "Epoch: 140, Loss: 0.0144, Train Acc: 1.0000, Val Acc: 0.4220, Test Acc: 0.4300\n",
      "Epoch: 150, Loss: 0.0139, Train Acc: 1.0000, Val Acc: 0.4320, Test Acc: 0.4330\n",
      "Epoch: 160, Loss: 0.0135, Train Acc: 1.0000, Val Acc: 0.4400, Test Acc: 0.4400\n",
      "Epoch: 170, Loss: 0.0130, Train Acc: 1.0000, Val Acc: 0.4400, Test Acc: 0.4420\n",
      "Epoch: 180, Loss: 0.0126, Train Acc: 1.0000, Val Acc: 0.4500, Test Acc: 0.4460\n",
      "Epoch: 190, Loss: 0.0122, Train Acc: 1.0000, Val Acc: 0.4580, Test Acc: 0.4510\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "# Load the Citeseer dataset\n",
    "dataset = Planetoid(root = 'Citeseer', name = 'Citeseer')\n",
    "data = dataset[0]\n",
    "\n",
    "# Step 1: Precompute normalized adjacency matrix\n",
    "def normalize_adjacency(edge_index, num_nodes):\n",
    "    # Add self-loops to the adjacency matrix\n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "    \n",
    "    # Compute degree of each node\n",
    "    row, col = edge_index\n",
    "    deg = degree(row, num_nodes=num_nodes)\n",
    "    \n",
    "    # Compute D^(-1/2)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0  # Handle zero degree\n",
    "    \n",
    "    # Normalize adjacency matrix: D^(-1/2) A D^(-1/2)\n",
    "    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "    return edge_index, norm\n",
    "\n",
    "edge_index, norm = normalize_adjacency(data.edge_index, data.num_nodes)\n",
    "\n",
    "# Step 2: Define GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, norm):\n",
    "        # First layer: H = ReLU(D^(-1/2) A D^(-1/2) X W1)\n",
    "        x = torch.mm(x, self.W1)\n",
    "        row, col = edge_index\n",
    "        x = torch.sparse.mm(torch.sparse_coo_tensor(edge_index, norm, (data.num_nodes, data.num_nodes)), x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Second layer: H = D^(-1/2) A D^(-1/2) H W2\n",
    "        x = torch.mm(x, self.W2)\n",
    "        x = torch.sparse.mm(torch.sparse_coo_tensor(edge_index, norm, (data.num_nodes, data.num_nodes)), x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, edge_index, norm)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Testing loop\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, edge_index, norm), []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "# Training and evaluation\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.linear(x)\n",
    "        x = torch.mm(adj, x)\n",
    "        return x\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNLayer(in_features, hidden_features)\n",
    "        self.conv2 = GCNLayer(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.conv1(x, adj)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(features.shape[1], 16, len(le.classes_))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj_matrix)\n",
    "    loss = F.nll_loss(output[train_mask], tensor_encoded_labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj_matrix)\n",
    "    preds = output.argmax(dim=1)\n",
    "    train_correct = preds[train_mask] == tensor_encoded_labels[train_mask]\n",
    "    val_correct = preds[val_mask] == tensor_encoded_labels[val_mask]\n",
    "    test_correct = preds[test_mask] == tensor_encoded_labels[test_mask]\n",
    "    \n",
    "    train_acc = train_correct.sum().item() / train_mask.sum().item()\n",
    "    val_acc = val_correct.sum().item() / val_mask.sum().item()\n",
    "    test_acc = test_correct.sum().item() / test_mask.sum().item()\n",
    "    \n",
    "    return train_acc, val_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.9253, Train Acc: 0.4161, Val Acc: 0.4240, Test Acc: 0.4373\n",
      "Epoch: 001, Loss: 1.8598, Train Acc: 0.4520, Val Acc: 0.4562, Test Acc: 0.4539\n",
      "Epoch: 002, Loss: 1.7820, Train Acc: 0.4813, Val Acc: 0.4839, Test Acc: 0.4779\n",
      "Epoch: 003, Loss: 1.6954, Train Acc: 0.5003, Val Acc: 0.4977, Test Acc: 0.4889\n",
      "Epoch: 004, Loss: 1.6063, Train Acc: 0.5146, Val Acc: 0.5069, Test Acc: 0.5074\n",
      "Epoch: 005, Loss: 1.5167, Train Acc: 0.5434, Val Acc: 0.5207, Test Acc: 0.5351\n",
      "Epoch: 006, Loss: 1.4265, Train Acc: 0.5798, Val Acc: 0.5438, Test Acc: 0.5590\n",
      "Epoch: 007, Loss: 1.3367, Train Acc: 0.6198, Val Acc: 0.5668, Test Acc: 0.6107\n",
      "Epoch: 008, Loss: 1.2482, Train Acc: 0.6752, Val Acc: 0.6498, Test Acc: 0.6605\n",
      "Epoch: 009, Loss: 1.1608, Train Acc: 0.7245, Val Acc: 0.6912, Test Acc: 0.6937\n",
      "Epoch: 010, Loss: 1.0744, Train Acc: 0.7737, Val Acc: 0.7189, Test Acc: 0.7306\n",
      "Epoch: 011, Loss: 0.9899, Train Acc: 0.8061, Val Acc: 0.7558, Test Acc: 0.7620\n",
      "Epoch: 012, Loss: 0.9086, Train Acc: 0.8338, Val Acc: 0.7696, Test Acc: 0.7989\n",
      "Epoch: 013, Loss: 0.8318, Train Acc: 0.8548, Val Acc: 0.7834, Test Acc: 0.8192\n",
      "Epoch: 014, Loss: 0.7600, Train Acc: 0.8722, Val Acc: 0.7880, Test Acc: 0.8266\n",
      "Epoch: 015, Loss: 0.6938, Train Acc: 0.8840, Val Acc: 0.7972, Test Acc: 0.8303\n",
      "Epoch: 016, Loss: 0.6334, Train Acc: 0.8871, Val Acc: 0.8203, Test Acc: 0.8358\n",
      "Epoch: 017, Loss: 0.5789, Train Acc: 0.8912, Val Acc: 0.8249, Test Acc: 0.8432\n",
      "Epoch: 018, Loss: 0.5303, Train Acc: 0.8969, Val Acc: 0.8433, Test Acc: 0.8450\n",
      "Epoch: 019, Loss: 0.4875, Train Acc: 0.8979, Val Acc: 0.8433, Test Acc: 0.8506\n",
      "Epoch: 020, Loss: 0.4500, Train Acc: 0.9041, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 021, Loss: 0.4175, Train Acc: 0.9076, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 022, Loss: 0.3893, Train Acc: 0.9082, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 023, Loss: 0.3649, Train Acc: 0.9123, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 024, Loss: 0.3435, Train Acc: 0.9153, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 025, Loss: 0.3247, Train Acc: 0.9179, Val Acc: 0.8479, Test Acc: 0.8561\n",
      "Epoch: 026, Loss: 0.3081, Train Acc: 0.9225, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 027, Loss: 0.2935, Train Acc: 0.9246, Val Acc: 0.8479, Test Acc: 0.8542\n",
      "Epoch: 028, Loss: 0.2804, Train Acc: 0.9277, Val Acc: 0.8525, Test Acc: 0.8542\n",
      "Epoch: 029, Loss: 0.2687, Train Acc: 0.9307, Val Acc: 0.8525, Test Acc: 0.8561\n",
      "Epoch: 030, Loss: 0.2580, Train Acc: 0.9333, Val Acc: 0.8479, Test Acc: 0.8561\n",
      "Epoch: 031, Loss: 0.2484, Train Acc: 0.9333, Val Acc: 0.8525, Test Acc: 0.8561\n",
      "Epoch: 032, Loss: 0.2396, Train Acc: 0.9364, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 033, Loss: 0.2315, Train Acc: 0.9384, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 034, Loss: 0.2241, Train Acc: 0.9415, Val Acc: 0.8525, Test Acc: 0.8598\n",
      "Epoch: 035, Loss: 0.2174, Train Acc: 0.9441, Val Acc: 0.8525, Test Acc: 0.8635\n",
      "Epoch: 036, Loss: 0.2112, Train Acc: 0.9456, Val Acc: 0.8525, Test Acc: 0.8635\n",
      "Epoch: 037, Loss: 0.2055, Train Acc: 0.9487, Val Acc: 0.8525, Test Acc: 0.8653\n",
      "Epoch: 038, Loss: 0.2002, Train Acc: 0.9507, Val Acc: 0.8525, Test Acc: 0.8653\n",
      "Epoch: 039, Loss: 0.1953, Train Acc: 0.9507, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 040, Loss: 0.1909, Train Acc: 0.9523, Val Acc: 0.8525, Test Acc: 0.8598\n",
      "Epoch: 041, Loss: 0.1867, Train Acc: 0.9538, Val Acc: 0.8525, Test Acc: 0.8579\n",
      "Epoch: 042, Loss: 0.1829, Train Acc: 0.9543, Val Acc: 0.8571, Test Acc: 0.8598\n",
      "Epoch: 043, Loss: 0.1793, Train Acc: 0.9554, Val Acc: 0.8571, Test Acc: 0.8598\n",
      "Epoch: 044, Loss: 0.1759, Train Acc: 0.9564, Val Acc: 0.8571, Test Acc: 0.8598\n",
      "Epoch: 045, Loss: 0.1728, Train Acc: 0.9579, Val Acc: 0.8571, Test Acc: 0.8616\n",
      "Epoch: 046, Loss: 0.1698, Train Acc: 0.9579, Val Acc: 0.8525, Test Acc: 0.8653\n",
      "Epoch: 047, Loss: 0.1671, Train Acc: 0.9605, Val Acc: 0.8571, Test Acc: 0.8635\n",
      "Epoch: 048, Loss: 0.1646, Train Acc: 0.9620, Val Acc: 0.8571, Test Acc: 0.8635\n",
      "Epoch: 049, Loss: 0.1623, Train Acc: 0.9625, Val Acc: 0.8571, Test Acc: 0.8616\n",
      "Epoch: 050, Loss: 0.1601, Train Acc: 0.9641, Val Acc: 0.8525, Test Acc: 0.8598\n",
      "Epoch: 051, Loss: 0.1580, Train Acc: 0.9641, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 052, Loss: 0.1561, Train Acc: 0.9651, Val Acc: 0.8525, Test Acc: 0.8635\n",
      "Epoch: 053, Loss: 0.1543, Train Acc: 0.9646, Val Acc: 0.8525, Test Acc: 0.8635\n",
      "Epoch: 054, Loss: 0.1526, Train Acc: 0.9651, Val Acc: 0.8525, Test Acc: 0.8653\n",
      "Epoch: 055, Loss: 0.1509, Train Acc: 0.9677, Val Acc: 0.8525, Test Acc: 0.8653\n",
      "Epoch: 056, Loss: 0.1493, Train Acc: 0.9677, Val Acc: 0.8525, Test Acc: 0.8653\n",
      "Epoch: 057, Loss: 0.1477, Train Acc: 0.9682, Val Acc: 0.8525, Test Acc: 0.8635\n",
      "Epoch: 058, Loss: 0.1463, Train Acc: 0.9687, Val Acc: 0.8525, Test Acc: 0.8635\n",
      "Epoch: 059, Loss: 0.1448, Train Acc: 0.9687, Val Acc: 0.8433, Test Acc: 0.8635\n",
      "Epoch: 060, Loss: 0.1434, Train Acc: 0.9687, Val Acc: 0.8433, Test Acc: 0.8635\n",
      "Epoch: 061, Loss: 0.1421, Train Acc: 0.9692, Val Acc: 0.8433, Test Acc: 0.8635\n",
      "Epoch: 062, Loss: 0.1408, Train Acc: 0.9702, Val Acc: 0.8387, Test Acc: 0.8653\n",
      "Epoch: 063, Loss: 0.1395, Train Acc: 0.9702, Val Acc: 0.8387, Test Acc: 0.8653\n",
      "Epoch: 064, Loss: 0.1383, Train Acc: 0.9713, Val Acc: 0.8433, Test Acc: 0.8653\n",
      "Epoch: 065, Loss: 0.1370, Train Acc: 0.9713, Val Acc: 0.8433, Test Acc: 0.8653\n",
      "Epoch: 066, Loss: 0.1358, Train Acc: 0.9713, Val Acc: 0.8433, Test Acc: 0.8635\n",
      "Epoch: 067, Loss: 0.1346, Train Acc: 0.9723, Val Acc: 0.8433, Test Acc: 0.8635\n",
      "Epoch: 068, Loss: 0.1335, Train Acc: 0.9728, Val Acc: 0.8433, Test Acc: 0.8616\n",
      "Epoch: 069, Loss: 0.1323, Train Acc: 0.9728, Val Acc: 0.8433, Test Acc: 0.8616\n",
      "Epoch: 070, Loss: 0.1312, Train Acc: 0.9723, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 071, Loss: 0.1300, Train Acc: 0.9733, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 072, Loss: 0.1289, Train Acc: 0.9738, Val Acc: 0.8525, Test Acc: 0.8598\n",
      "Epoch: 073, Loss: 0.1278, Train Acc: 0.9743, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 074, Loss: 0.1268, Train Acc: 0.9743, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 075, Loss: 0.1257, Train Acc: 0.9743, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 076, Loss: 0.1247, Train Acc: 0.9738, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 077, Loss: 0.1237, Train Acc: 0.9743, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 078, Loss: 0.1226, Train Acc: 0.9749, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 079, Loss: 0.1217, Train Acc: 0.9759, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 080, Loss: 0.1207, Train Acc: 0.9764, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 081, Loss: 0.1197, Train Acc: 0.9764, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 082, Loss: 0.1188, Train Acc: 0.9764, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 083, Loss: 0.1178, Train Acc: 0.9769, Val Acc: 0.8433, Test Acc: 0.8616\n",
      "Epoch: 084, Loss: 0.1169, Train Acc: 0.9774, Val Acc: 0.8433, Test Acc: 0.8635\n",
      "Epoch: 085, Loss: 0.1160, Train Acc: 0.9774, Val Acc: 0.8433, Test Acc: 0.8635\n",
      "Epoch: 086, Loss: 0.1152, Train Acc: 0.9779, Val Acc: 0.8479, Test Acc: 0.8653\n",
      "Epoch: 087, Loss: 0.1143, Train Acc: 0.9785, Val Acc: 0.8479, Test Acc: 0.8653\n",
      "Epoch: 088, Loss: 0.1135, Train Acc: 0.9785, Val Acc: 0.8479, Test Acc: 0.8653\n",
      "Epoch: 089, Loss: 0.1126, Train Acc: 0.9785, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 090, Loss: 0.1118, Train Acc: 0.9790, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 091, Loss: 0.1110, Train Acc: 0.9790, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 092, Loss: 0.1103, Train Acc: 0.9790, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 093, Loss: 0.1095, Train Acc: 0.9795, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 094, Loss: 0.1087, Train Acc: 0.9800, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 095, Loss: 0.1080, Train Acc: 0.9800, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 096, Loss: 0.1073, Train Acc: 0.9800, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 097, Loss: 0.1065, Train Acc: 0.9805, Val Acc: 0.8479, Test Acc: 0.8635\n",
      "Epoch: 098, Loss: 0.1058, Train Acc: 0.9805, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 099, Loss: 0.1051, Train Acc: 0.9805, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 100, Loss: 0.1045, Train Acc: 0.9805, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 101, Loss: 0.1038, Train Acc: 0.9805, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 102, Loss: 0.1031, Train Acc: 0.9810, Val Acc: 0.8479, Test Acc: 0.8616\n",
      "Epoch: 103, Loss: 0.1025, Train Acc: 0.9810, Val Acc: 0.8479, Test Acc: 0.8616\n",
      "Epoch: 104, Loss: 0.1019, Train Acc: 0.9810, Val Acc: 0.8479, Test Acc: 0.8616\n",
      "Epoch: 105, Loss: 0.1013, Train Acc: 0.9810, Val Acc: 0.8479, Test Acc: 0.8616\n",
      "Epoch: 106, Loss: 0.1006, Train Acc: 0.9810, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 107, Loss: 0.1000, Train Acc: 0.9815, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 108, Loss: 0.0995, Train Acc: 0.9820, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 109, Loss: 0.0989, Train Acc: 0.9820, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 110, Loss: 0.0983, Train Acc: 0.9820, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 111, Loss: 0.0977, Train Acc: 0.9820, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 112, Loss: 0.0972, Train Acc: 0.9820, Val Acc: 0.8525, Test Acc: 0.8616\n",
      "Epoch: 113, Loss: 0.0966, Train Acc: 0.9820, Val Acc: 0.8525, Test Acc: 0.8598\n",
      "Epoch: 114, Loss: 0.0961, Train Acc: 0.9820, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 115, Loss: 0.0956, Train Acc: 0.9820, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 116, Loss: 0.0951, Train Acc: 0.9820, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 117, Loss: 0.0946, Train Acc: 0.9820, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 118, Loss: 0.0940, Train Acc: 0.9820, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 119, Loss: 0.0936, Train Acc: 0.9820, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 120, Loss: 0.0931, Train Acc: 0.9826, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 121, Loss: 0.0926, Train Acc: 0.9826, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 122, Loss: 0.0921, Train Acc: 0.9831, Val Acc: 0.8479, Test Acc: 0.8616\n",
      "Epoch: 123, Loss: 0.0916, Train Acc: 0.9836, Val Acc: 0.8479, Test Acc: 0.8616\n",
      "Epoch: 124, Loss: 0.0912, Train Acc: 0.9841, Val Acc: 0.8479, Test Acc: 0.8616\n",
      "Epoch: 125, Loss: 0.0907, Train Acc: 0.9841, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 126, Loss: 0.0902, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 127, Loss: 0.0898, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 128, Loss: 0.0894, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 129, Loss: 0.0889, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 130, Loss: 0.0885, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 131, Loss: 0.0881, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 132, Loss: 0.0877, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 133, Loss: 0.0872, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 134, Loss: 0.0868, Train Acc: 0.9846, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 135, Loss: 0.0864, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 136, Loss: 0.0860, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 137, Loss: 0.0857, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 138, Loss: 0.0853, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 139, Loss: 0.0849, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8542\n",
      "Epoch: 140, Loss: 0.0845, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8542\n",
      "Epoch: 141, Loss: 0.0841, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 142, Loss: 0.0838, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 143, Loss: 0.0834, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 144, Loss: 0.0830, Train Acc: 0.9846, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 145, Loss: 0.0827, Train Acc: 0.9851, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 146, Loss: 0.0823, Train Acc: 0.9856, Val Acc: 0.8479, Test Acc: 0.8561\n",
      "Epoch: 147, Loss: 0.0820, Train Acc: 0.9856, Val Acc: 0.8479, Test Acc: 0.8561\n",
      "Epoch: 148, Loss: 0.0816, Train Acc: 0.9861, Val Acc: 0.8479, Test Acc: 0.8561\n",
      "Epoch: 149, Loss: 0.0813, Train Acc: 0.9851, Val Acc: 0.8479, Test Acc: 0.8561\n",
      "Epoch: 150, Loss: 0.0810, Train Acc: 0.9856, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 151, Loss: 0.0806, Train Acc: 0.9861, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 152, Loss: 0.0803, Train Acc: 0.9861, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 153, Loss: 0.0800, Train Acc: 0.9861, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 154, Loss: 0.0797, Train Acc: 0.9861, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 155, Loss: 0.0793, Train Acc: 0.9867, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 156, Loss: 0.0790, Train Acc: 0.9867, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 157, Loss: 0.0787, Train Acc: 0.9867, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 158, Loss: 0.0784, Train Acc: 0.9861, Val Acc: 0.8479, Test Acc: 0.8579\n",
      "Epoch: 159, Loss: 0.0781, Train Acc: 0.9861, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 160, Loss: 0.0778, Train Acc: 0.9867, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 161, Loss: 0.0775, Train Acc: 0.9867, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 162, Loss: 0.0772, Train Acc: 0.9867, Val Acc: 0.8479, Test Acc: 0.8598\n",
      "Epoch: 163, Loss: 0.0770, Train Acc: 0.9867, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 164, Loss: 0.0767, Train Acc: 0.9867, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 165, Loss: 0.0764, Train Acc: 0.9872, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 166, Loss: 0.0761, Train Acc: 0.9872, Val Acc: 0.8433, Test Acc: 0.8598\n",
      "Epoch: 167, Loss: 0.0758, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 168, Loss: 0.0756, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 169, Loss: 0.0753, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 170, Loss: 0.0750, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 171, Loss: 0.0747, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 172, Loss: 0.0745, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8579\n",
      "Epoch: 173, Loss: 0.0742, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 174, Loss: 0.0740, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 175, Loss: 0.0737, Train Acc: 0.9877, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 176, Loss: 0.0735, Train Acc: 0.9882, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 177, Loss: 0.0732, Train Acc: 0.9882, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 178, Loss: 0.0730, Train Acc: 0.9882, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 179, Loss: 0.0727, Train Acc: 0.9887, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 180, Loss: 0.0725, Train Acc: 0.9887, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 181, Loss: 0.0722, Train Acc: 0.9887, Val Acc: 0.8433, Test Acc: 0.8561\n",
      "Epoch: 182, Loss: 0.0720, Train Acc: 0.9887, Val Acc: 0.8433, Test Acc: 0.8542\n",
      "Epoch: 183, Loss: 0.0718, Train Acc: 0.9887, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 184, Loss: 0.0715, Train Acc: 0.9892, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 185, Loss: 0.0713, Train Acc: 0.9892, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 186, Loss: 0.0711, Train Acc: 0.9892, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 187, Loss: 0.0708, Train Acc: 0.9892, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 188, Loss: 0.0706, Train Acc: 0.9892, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 189, Loss: 0.0704, Train Acc: 0.9892, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 190, Loss: 0.0701, Train Acc: 0.9892, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 191, Loss: 0.0699, Train Acc: 0.9892, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 192, Loss: 0.0697, Train Acc: 0.9897, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 193, Loss: 0.0695, Train Acc: 0.9897, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 194, Loss: 0.0693, Train Acc: 0.9897, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 195, Loss: 0.0691, Train Acc: 0.9897, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 196, Loss: 0.0689, Train Acc: 0.9897, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 197, Loss: 0.0687, Train Acc: 0.9897, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 198, Loss: 0.0685, Train Acc: 0.9897, Val Acc: 0.8387, Test Acc: 0.8542\n",
      "Epoch: 199, Loss: 0.0683, Train Acc: 0.9897, Val Acc: 0.8387, Test Acc: 0.8524\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
