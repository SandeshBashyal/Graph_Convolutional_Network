{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Classification with Graph Convolutional Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "\n",
    "dataset = Planetoid(root = 'Citeseer', name = 'Citeseer')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.7864, Train Acc: 0.8917, Val Acc: 0.4940, Test Acc: 0.5070\n",
      "Epoch: 010, Loss: 0.1647, Train Acc: 1.0000, Val Acc: 0.6800, Test Acc: 0.6680\n",
      "Epoch: 020, Loss: 0.0193, Train Acc: 1.0000, Val Acc: 0.6720, Test Acc: 0.6650\n",
      "Epoch: 030, Loss: 0.0072, Train Acc: 1.0000, Val Acc: 0.6760, Test Acc: 0.6580\n",
      "Epoch: 040, Loss: 0.0067, Train Acc: 1.0000, Val Acc: 0.6740, Test Acc: 0.6720\n",
      "Epoch: 050, Loss: 0.0090, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6790\n",
      "Epoch: 060, Loss: 0.0113, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6760\n",
      "Epoch: 070, Loss: 0.0121, Train Acc: 1.0000, Val Acc: 0.6920, Test Acc: 0.6800\n",
      "Epoch: 080, Loss: 0.0116, Train Acc: 1.0000, Val Acc: 0.6920, Test Acc: 0.6790\n",
      "Epoch: 090, Loss: 0.0108, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.6820\n",
      "Epoch: 100, Loss: 0.0103, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.6830\n",
      "Epoch: 110, Loss: 0.0097, Train Acc: 1.0000, Val Acc: 0.6900, Test Acc: 0.6830\n",
      "Epoch: 120, Loss: 0.0093, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.6830\n",
      "Epoch: 130, Loss: 0.0089, Train Acc: 1.0000, Val Acc: 0.6900, Test Acc: 0.6820\n",
      "Epoch: 140, Loss: 0.0085, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6840\n",
      "Epoch: 150, Loss: 0.0082, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6850\n",
      "Epoch: 160, Loss: 0.0079, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6820\n",
      "Epoch: 170, Loss: 0.0077, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6830\n",
      "Epoch: 180, Loss: 0.0074, Train Acc: 1.0000, Val Acc: 0.6860, Test Acc: 0.6830\n",
      "Epoch: 190, Loss: 0.0072, Train Acc: 1.0000, Val Acc: 0.6860, Test Acc: 0.6840\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Step 1: Load the Citeseer Dataset\n",
    "dataset = Planetoid(root = 'Citeseer', name = 'Citeseer')\n",
    "# Step 2: Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and other utilities\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Step 3: Training Loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Step 4: Test Function\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data), []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "# Training and Evaluation\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 11.0108, Train Acc: 0.1583, Val Acc: 0.1720, Test Acc: 0.1960\n",
      "Epoch: 010, Loss: 1.7516, Train Acc: 0.7000, Val Acc: 0.2840, Test Acc: 0.3230\n",
      "Epoch: 020, Loss: 0.2621, Train Acc: 0.9333, Val Acc: 0.3560, Test Acc: 0.4030\n",
      "Epoch: 030, Loss: 0.0431, Train Acc: 1.0000, Val Acc: 0.3860, Test Acc: 0.4190\n",
      "Epoch: 040, Loss: 0.0243, Train Acc: 1.0000, Val Acc: 0.3880, Test Acc: 0.4240\n",
      "Epoch: 050, Loss: 0.0183, Train Acc: 1.0000, Val Acc: 0.3960, Test Acc: 0.4290\n",
      "Epoch: 060, Loss: 0.0168, Train Acc: 1.0000, Val Acc: 0.3880, Test Acc: 0.4300\n",
      "Epoch: 070, Loss: 0.0167, Train Acc: 1.0000, Val Acc: 0.3800, Test Acc: 0.4250\n",
      "Epoch: 080, Loss: 0.0170, Train Acc: 1.0000, Val Acc: 0.3860, Test Acc: 0.4260\n",
      "Epoch: 090, Loss: 0.0170, Train Acc: 1.0000, Val Acc: 0.3920, Test Acc: 0.4270\n",
      "Epoch: 100, Loss: 0.0167, Train Acc: 1.0000, Val Acc: 0.3960, Test Acc: 0.4240\n",
      "Epoch: 110, Loss: 0.0162, Train Acc: 1.0000, Val Acc: 0.3960, Test Acc: 0.4240\n",
      "Epoch: 120, Loss: 0.0156, Train Acc: 1.0000, Val Acc: 0.4060, Test Acc: 0.4240\n",
      "Epoch: 130, Loss: 0.0150, Train Acc: 1.0000, Val Acc: 0.4160, Test Acc: 0.4300\n",
      "Epoch: 140, Loss: 0.0144, Train Acc: 1.0000, Val Acc: 0.4220, Test Acc: 0.4300\n",
      "Epoch: 150, Loss: 0.0139, Train Acc: 1.0000, Val Acc: 0.4320, Test Acc: 0.4330\n",
      "Epoch: 160, Loss: 0.0135, Train Acc: 1.0000, Val Acc: 0.4400, Test Acc: 0.4400\n",
      "Epoch: 170, Loss: 0.0130, Train Acc: 1.0000, Val Acc: 0.4400, Test Acc: 0.4420\n",
      "Epoch: 180, Loss: 0.0126, Train Acc: 1.0000, Val Acc: 0.4500, Test Acc: 0.4460\n",
      "Epoch: 190, Loss: 0.0122, Train Acc: 1.0000, Val Acc: 0.4580, Test Acc: 0.4510\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Precompute normalized adjacency matrix\n",
    "def normalize_adjacency(edge_index, num_nodes):\n",
    "    # Add self-loops to the adjacency matrix\n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "    \n",
    "    # Compute degree of each node\n",
    "    row, col = edge_index\n",
    "    deg = degree(row, num_nodes=num_nodes)\n",
    "    \n",
    "    # Compute D^(-1/2)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0  # Handle zero degree\n",
    "    \n",
    "    # Normalize adjacency matrix: D^(-1/2) A D^(-1/2)\n",
    "    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "    return edge_index, norm\n",
    "\n",
    "edge_index, norm = normalize_adjacency(data.edge_index, data.num_nodes)\n",
    "\n",
    "# Step 2: Define GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.W1 = torch.nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
    "        self.W2 = torch.nn.Parameter(torch.randn(hidden_dim, output_dim))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, norm):\n",
    "        # First layer: H = ReLU(D^(-1/2) A D^(-1/2) X W1)\n",
    "        x = torch.mm(x, self.W1)\n",
    "        row, col = edge_index\n",
    "        x = torch.sparse.mm(torch.sparse_coo_tensor(edge_index, norm, (data.num_nodes, data.num_nodes)), x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Second layer: H = D^(-1/2) A D^(-1/2) H W2\n",
    "        x = torch.mm(x, self.W2)\n",
    "        x = torch.sparse.mm(torch.sparse_coo_tensor(edge_index, norm, (data.num_nodes, data.num_nodes)), x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, edge_index, norm)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Testing loop\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, edge_index, norm), []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "# Training and evaluation\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.7879, Train Acc: 0.9250, Val Acc: 0.4940, Test Acc: 0.5340\n",
      "Epoch: 010, Loss: 0.1138, Train Acc: 1.0000, Val Acc: 0.6760, Test Acc: 0.6800\n",
      "Epoch: 020, Loss: 0.0135, Train Acc: 1.0000, Val Acc: 0.6680, Test Acc: 0.6710\n",
      "Epoch: 030, Loss: 0.0057, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6710\n",
      "Epoch: 040, Loss: 0.0058, Train Acc: 1.0000, Val Acc: 0.6800, Test Acc: 0.6760\n",
      "Epoch: 050, Loss: 0.0080, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.6830\n",
      "Epoch: 060, Loss: 0.0103, Train Acc: 1.0000, Val Acc: 0.6880, Test Acc: 0.6870\n",
      "Epoch: 070, Loss: 0.0112, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6880\n",
      "Epoch: 080, Loss: 0.0109, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6870\n",
      "Epoch: 090, Loss: 0.0103, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6820\n",
      "Epoch: 100, Loss: 0.0097, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6830\n",
      "Epoch: 110, Loss: 0.0093, Train Acc: 1.0000, Val Acc: 0.6800, Test Acc: 0.6820\n",
      "Epoch: 120, Loss: 0.0089, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6810\n",
      "Epoch: 130, Loss: 0.0085, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6830\n",
      "Epoch: 140, Loss: 0.0082, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6850\n",
      "Epoch: 150, Loss: 0.0079, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6850\n",
      "Epoch: 160, Loss: 0.0076, Train Acc: 1.0000, Val Acc: 0.6840, Test Acc: 0.6860\n",
      "Epoch: 170, Loss: 0.0074, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6830\n",
      "Epoch: 180, Loss: 0.0072, Train Acc: 1.0000, Val Acc: 0.6820, Test Acc: 0.6840\n",
      "Epoch: 190, Loss: 0.0070, Train Acc: 1.0000, Val Acc: 0.6860, Test Acc: 0.6840\n",
      "Best Validation Accuracy: 0.6900, Test Accuracy at Best Val: 0.6810\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Precompute normalized adjacency matrix\n",
    "def normalize_adjacency(edge_index, num_nodes):\n",
    "    edge_index, _ = add_self_loops(edge_index, num_nodes=num_nodes)\n",
    "    row, col = edge_index\n",
    "    deg = degree(row, num_nodes=num_nodes, dtype=torch.float32)\n",
    "    deg_inv_sqrt = deg.pow(-0.5)\n",
    "    deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "    norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "    return edge_index, norm\n",
    "\n",
    "edge_index, norm = normalize_adjacency(data.edge_index, data.num_nodes)\n",
    "\n",
    "# Step 2: Define GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.W1 = torch.nn.Parameter(torch.empty(input_dim, hidden_dim))\n",
    "        self.W2 = torch.nn.Parameter(torch.empty(hidden_dim, output_dim))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.W1)\n",
    "        torch.nn.init.xavier_uniform_(self.W2)\n",
    "\n",
    "    def forward(self, x, edge_index, norm):\n",
    "        # Layer 1\n",
    "        x = torch.mm(x, self.W1)\n",
    "        row, col = edge_index\n",
    "        x = torch.sparse.mm(torch.sparse_coo_tensor(edge_index, norm, (x.size(0), x.size(0))), x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = torch.mm(x, self.W2)\n",
    "        x = torch.sparse.mm(torch.sparse_coo_tensor(edge_index, norm, (x.size(0), x.size(0))), x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model, optimizer, and data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(input_dim=dataset.num_node_features, hidden_dim=64, output_dim=dataset.num_classes).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, edge_index, norm)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Testing loop\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits = model(data.x, edge_index, norm)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "# Training and evaluation\n",
    "best_val_acc = 0\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'Best Validation Accuracy: {best_val_acc:.4f}, Test Accuracy at Best Val: {best_test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
